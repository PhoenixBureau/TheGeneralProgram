
    The Egg of Ouroboros

    A Notation for Tractable Reasoning



-



    Know names to find the Ten Thousand Things
    No names am the Way

        ~the Old Man


    In the beginning was the Void,
    And the Void was without Form.

-

There are three rewards to be won by eating the Egg of Ouroboros.

The first and most important (indeed the only import) is to effect transcendence of the illusion of form.

The second is a notation for universal reasoning that is simple, elegant and direct.  This notation provides a mechanism for constructing all possible universes and, therefore, our own.  It is the power of this notation to permit the prepared mind to approach math as scripture and read the Word of God in the Libre Mundi.

Third, new results in circuit design and program construction are enabled.


Nonsensical Formalities
________________________________________


In the absence of all distinction nothing can be signified.

To make a distinction is to create the entire Universe, complete, Eternal, as it is now and ever shall be.  The "Word of God" is fractally encoded into the structure of structure at the most intrinsic and essential level.  Every tiniest least distinction made inherently contains or predicts or necessitates the entirety of everything you experience and have experienced and will experience, as well as the experiences of all other sentient beings everywhere throughout time.


- -


Observe an empty expanse.  For concreteness take the area below this sentence as a small space of the nondistinct Void.  (O Beloved Reader if you are without the visual sense the following can all be done in sound as well.)




There, that was a little bit of the Void, without distinction.

Now consider this mark:

   .


That mark can be said to be "one thing", yes?  But a moment's reflection will let us understand there are three "things there":

1 The inside of the mark.
2 The outside of the mark.
3 The distinction, the boundary, between inside and outside.


- -


Having made any distinction the Universe is partitioned into two "spaces" and the boundary between them. We can naturally represent this situation with a closed figure such as a circle:

    O


But having done this to illustrate the triune nature of any distinction we can reflect again that this figure O has five parts: the inside and outside, the "inside" of the boundary, and the two boundaries separating the "inside" of the boundary from the outside-outside and the inside-outside.

This is a completely natural process.  Consider your location on the Earth.  The great majority of your life takes place on just such a boundary between the lava inward and the hard vacuum outward, and again on the boundary between the frozen-stone crust and the vaporous atmosphere.  If you stoop and examine conditions very close to this boundary you will discover additional boundaries to the limits of your perceptual apparatus.

This is a general phenomenon: the closer a sensory apparatus approaches in scale to a boundary, the more meta-boundaries it will resolve, up to its limits.


- -

Make a distinction,

    .


Notice that it partitions the Universe into the mark and the unmarked.

    O


We can imagine taking a perceptual position within the boundary itself and looking "outward" both to the distinct "inside-outside" and the "outside-outside" and perceiving "two things".  We can say that the Sky is around the Earth:

    (O)


Or we can say that the Earth and the Sky are "two different things":

    OO


Either way, the boundary is the perceptual illusion that we name "the horizon".  A moment's reflection lets us understand that "the horizon" is not real except as "it" is perceived.  Continuing in this way, we discover that there "are" many "things" that only exist because we perceive and name them.  Continuing in this way, we discover that all "things" only exist because we perceive and name them.  If we neglect to distinguish and name "things" there is "no-thing" there.


A man named George Spencer-Brown developed a wonderful notation and wrote it in a book he called "Laws of Form".  That is where I first encountered it, from a reference in the Whole Earth Catalog.


If you are interested in the "mystical" aspects of this notation I urge you to read the Tao Te Ching (I recommend highly the translation by J. Star) and GSB's "Laws of Form".  The experiences of Dr. Jill Bolte Taylor are also of direct relevance.  A neuroanatomist who suffered a stroke, she was able to understand what was happening to her as the symbolic processing areas of her brain were impaired by the flooding blood.  During the time that her symbolic processing systems were mechanically disengaged, Dr. Taylor reports transcendent bliss and the non-ability to differentiate between "her" body and the "external" Universe of phenomenon.


No more will be said here about the transcendental function of the notation.


"Formal Nonsense"
________________________________________


For our purposes it will suffice to understand the following rule:

    A "mark" is a circle that is empty or contains no marks.


You may find it worthwhile to play with some circles and use the rule to determine if the resulting forms count as "marks" or not.  For example these forms (taking paired parentheses to indicate closed circles) are "marks":

    O    OOO     ((O))    OO


While these forms are not "marks" (or "not-marks"):


   (O)    (OO)    ((O)O)


William Bricken has shown that two rules for "rewriting" or "transforming" these forms permit all possible forms to be generated or elucidated without altering the "mark"-ness of the resulting new forms.

These are the two rules:

          O <===> OO

    nothing <===> (O)
    

These rules can be described in language in many ways. We can say that any mark is the same as any number of marks, and that the Void is the same as a non-mark.  Using the rules from left-to-right (in the above depiction) generates new forms, and using them right-to-left reduces forms back to either a mark or the Void.



In the Land of Nom
________________________________________


Once we can generate myriad forms we can perform another trick that is somewhat mysterious and somewhat concrete (like the primal distinction.)  The name of this trick is "naming", and it is, in a sense, the opposite of Void.


The initial distinction can be considered a name for itself.  By existing (which it only does because you made it) the distinction the distinguishes something from the Void, and as there is no other thing to name, it names itself.


The Void is knowable, inexpressible, but every distinction is a self-creating "name".


Then we do something really special, we "let" one bit of the plenum over here "name" some other bit of the plenum over there.


One pattern of distinctions (which we can think about concretely using the circle language or others) can "stand for" another pattern. This is an arbitrary and meaningless thing to do, but once made such a linkage is more-or-less mechanical.


We can write dictionaries and train dogs to salivate at the sound of a certain bell.  (This later sort of naming is just at the inflection point between analog and digital computation, the dog "knows" a word but it could never give you a symbolic definition of it.  However, if you were somehow to "mention" the bell-sound in a way the dog could re-cognize, perhaps by beginning a motion identical to the motion you make to ring the bell in the absence of the actual bell, the dog would salivate.  Put another way, you can tell if the dog "thinks" you are "talking about" dinner-time by whether or not it salivates.)

Our own brains are constantly attempting to move pattern-handling from costly "thought" to cheap pattern-recognition.  We are always coming up with "names" for new parts of the plenum as we understand their relevance and utility for "our lives".


Despite the ocasional problems with this approach it has proved so useful (for certain kinds of "useful") that we have built machines to do it for us, faster and with greater range than our nervous systems can handle.  Computers are collections of "names" for automatic pattern manipulation, along with means for recombining the names to form new names.


We call these names "programs" and the art and science of writing new names out of the old is called "computer programming".


There are two ways of making names out of the circle language, and both are "cheating".


First, we can simple arrange patterns of circles to represent distinct symbols that we then "let stand for" other patterns.  This is how most kinds of printers actually work.  An "ink-jet" printer is precisely placing tiny circles of ink on the paper to make patterns that our nervous systems recognize.


The other way of naming patterns is to come up with some "ordering" convention for patterns, and then use those "numbers" to "index" any other sequence of patterns.


TODO: More on sequences, maybe mention Church Numerals.


We will choose the first method in most of what follows and say that a letter, such as 'a', may be the name of some other pattern of circles.


But here's the really weird, the really bizarre and unreasonable thing we proceed to do next, we don't say what pattern of circles the name is a name for yet.


Now, whether you realize it or not, we're so used to doing this weird trick so automatically that we tend to forget just how mind-bendingly bizarre and unusual it is.

It's one thing to say a mark "names itself" because that's not really saying anything at all, at all, is it? If you say that the mark is saying itself, have you said anything?  And, if you weren't there saying that would it the mark still be "a name"?  What kind of a question is that, anyway?

To say that a bit of pattern can "stand for" some other bit of pattern is, in comparison, quite reasonable.  "Smoke" means "fire" after all, doesn't it?


But when my friend says to me, "I have a freind, and I can't tell you their name, but..." don't I tend to know that there is a freind of my friend? But that this friend's friend could be my friend, really? But maybe they are just making this up, after all, I'm just making them up.


So let's say that my friend's friend once told my friend, or so my friend informs me, that the squiggly line 'a' "is a name for" some pattern of circles, but I never found out which pattern of circles it was.  I know that every pattern of circles can reduce down to either the Void or a ("the") mark, so I know this pattern "named" by 'a' must reduce down to one of those as well.

Consider this form:

    (a(a))

Even though I don't know what pattern of circles 'a' is a name for, I can still tell that this pattern of circles is Void-valued.  The easy way to think about a simple pattern like this one is to just imagine replacing 'a' wherever it appears with one of the two values (Void and Something Else) and then checking the result.

If 'a' is nothing at all then:

    (())

Which is Void-valued.  And if 'a' is a mark:

    (()(()))

Which is also Void-valued. So, no matter what 'a' might be (a(a)) is Void-valued.


It turns out that if we follow a few simple rules regarding names the resulting expressions can be made to enact something called the "Primary Logic".

The rules are simply:

    "a name must stand for the same pattern wherever it is used in a circle-and-name expression"

    a(b) = a(ab)




Many people, including George Spencer-Brown in "The Laws of Forms" have examined and explained this fanstatic and marvoulous result.  A man named George Burnett-Stuart has created a website "The Markable Mark" (see the references section below) that provides a really wonderful and smooth introduction to the world of symbolic logic and formal reasoning using the notation of the Laws of Form, and it is his work which directly stimulated the paper you're reading now (any blame is mine, the inspiration is all his.)


TODO: Explain a little bit more so people who can't or don't want to go work through the Markable Mark site can still, hopefully, follow what's going on below.


So, any expression of circles (and symbols representing circle expressions, and symbols representing expressions of such symbols and circle expressions) can be manipulated in value-preserving ways by known logic.  We can model and solve logical systems using the notation.


If this were all then we have not said much that is not in any book on logic and symbolic reasoning.  If the only advantage to the Circle Language were that it gives a simple and elegant introduction to formal reasoning then I would be satisfied, as that is a worthy and sorely needed thing.  Admirably, the notation also provides a means of constructing logical circuits and programs.


The resulting way of thinking about machines provides for a unified treatment of hardware and software, parallel and sequential operations, and self-acting (Cybernetic) systems.



In the Land of Nor
________________________________________


The key is to notice that a mark acts as a "NOR" gate on its contents.


A circle in the Circle Language "is":

    * A signal
    * The value of a signal
    * An operation.

A circle is both a value (term) and an action (name). This is a very subtle point and can be tricky to understand properly if you haven't encountered it before.  In the lambda calculus each lambda term is both a value and [the name for] an action to take on some values to derive or generate new values, which can themselves be lambda terms.  Likewise, in the SKI combinator calculus the combinators are both values and names of actions to take.  In computer programming, all programs are stored as sequences of bits, which are values.  The bits "name" the actions to take.

We can interpret the content-free Circle Language forms as networks of NOR (not-or) logic gates.  It is known art that all logic gate networks can be constructed out of NOR gates.  Therefore the Circle Language is a notation for digital logic circuits, and every digital logic circuit can be represented as a form of the Circle Language.

It is perhaps unnecessary to remark that all computers can be represented in the Circle Language.


- -


Each LoF mark form ("circle expression") is a specification of a network of logic gates and a proof of the expectable operation of that network. The proof proves the circuit and the circuit computes the proof.


Composing circle expressions composes the proofs and connects the circuits.


Two circle expressions that yield the same behaviour ("extensional identity") can have different properties otherwise (intentional dis-identity.)  This can be exploited to create programs and hardware with characteristics that are desirable while proving extensional identity with "correct" forms.  It should be possible to construct simple systems that search for extensionally identical forms of expressions automatically.  (cf. Gödel Machines.)


- -


Computers can be modeled as bit-sets (comprised of the registers, status bits, RAM, I/O, &c.) and a set of sets of circle expressions that define precisely transformations between "states" of the bits corresponding to the microcode &c. of the CPU circuitry in question.


There is no special treatment for multiple computers connected to each other as compared to the circuitry in one machine connected to itself.


The difference between a non-computer-machine and a computer is generally taken to be that, in a computer, the choice of which set of circle expressions to  evaluate over the bits is determined by the values of a certain subset of the bits (i.e. the "Program Counter" or "Instruction Pointer" or some such, and the contents of the program RAM pointed to by same.)


The fact that the resulting selected circle expressions will only treat a tiny subset (typically) of the the bits in the total bit-set is precisely the complaint against the Von Neumann "bottleneck" Backus made in his Turing award talk on Functional Programming.


The "bottleneck" is unnecessary. It is a notational artifact.


I don't wish to belabor this point but it does bear some emphasis.  We have known how to create "parallel" or "concurrent" systems in hardware for a long time.  An eight-bit machine is doing (more than) eight things at a time when it, for example, adds two bytes and stores the result in a register.  For no essential reason we give up this art when we write programs in assembly language (and all "higher-level" programming languages are arrangements of machine instructions in the same way that all material around you is an arrangment of atoms.)


We can easily compose large circle expressions that develop results across a larger subset of the bits in the bitset of a computer than those provided in the typical state-of-the-art machines.  In effect, RAM would be one (or a few) very large registers (mega-bits or more) and the Arithmetic-Logic-Unit would apply a myriad of expressions to all or most of the bits in each computational cycle to develop patterns closer to the desired output of the running program.


The task of programmers then becomes constructing correct and efficient circle expressions (which are just logic circuit schematics) that develop the desired bit-patterns in the registers.


One could imagine that no "output" as such would be needed: the circle expressions can be designed to construct output patterns directly in the register RAM, a portion of which would  be the system's video RAM  (cf. Turing Drawings.)  Alternately bit-patterns could be written to permanent storage or transmitted over a network.


- -


Circuits


The circle language can be interpreted as digital logic circuits.

    nor a, b -> (ab)
     or a, b -> ((ab))
    and a, b -> ((a)(b))
   nand a, b -> (((a)(b)))
    xor a, b -> (((a)(b))(ab))


It should be understood that more than two symbols may appear within a form.  The symbols are taken to stand for any other pattern of circles, or circles and symbols, as described above in the section on using circle expressions to represent the formal logics, but here they also represent the "inputs" to the digital circuits.  The expressions themselves are the "outputs".


For concreteness we will develop a model of the Circle Language using the Python computer programming language.


In Python, let "a form" be any data-structure composed entirely of tuples.


We shall consider the absence of any form to be "the ground" and have the Boolean value of True.  A "mark" is the empty tuple or any tuple without a mark in it, we shall consider marks to have the Boolean value of False.


The function "mark" reduces any form and returns its Boolean value.  In Python it can be implemented as:

    def mark(form):
      return not form or not any(mark(inner) for inner in form))


This is a direct "translation" of the sentence. "A mark is circle that is empty or has no marks in it."  In Python the empty tuple is considered to have the Boolean value of False so we invert that value.  If the tuple/form has contents then we recursively examine them to determine if any of them are marks.  In effect we are performing a depth-first "walk" of a tree, and "short-circuiting" the walk as soon as we can determine the "mark"-ness of the tuple/form.


There are several optimizations that could be made to this function, simple as it is.  We could "memoize" the function so that it did not re-compute the "mark"-ness of a form that it had "seen" before.  Also, it would be easy to manipulate our forms into a sort of standard form that ensured the "walk" terminated as early as possible.  For now we will neglect such considerations.


- -


We can implement a model of Binary Boolean Logic gates using the tuple-based form of the Circle Language with the following Python functions:

    nor = lambda *bits: bits
    or_ = lambda *bits: nor(bits)
    and_ = lambda *bits: tuple(nor(bit) for bit in bits)
    nand = lambda *bits: nor(and_(*bits))
    xor = lambda *bits: nor(and_(*bits), nor(*bits))


If we use the above functions to generate circle expressions for the symbols 'a' and 'b' we can examine their forms:

    a, b = 'ab'

    nor(a, b) -> ('a', 'b')
    or(a, b) -> (('a', 'b'),)
    and(a, b) -> (('a',), ('b',))
    nand(a, b) -> ((('a',), ('b',)),)
    xor(a, b) -> ((('a',), ('b',)), ('a', 'b'))


Here are truth tables generated from the above definitions by means of substituting the values in the first two columns (as tuple mark () and not-mark ((),) values for Boolean values) into the above expressions and then reducing them by means of the "mark" function.

     and_
    ------
     00|0
     01|0
     10|0
     11|1

     or_
    ------
     00|0
     01|1
     10|1
     11|1

     nand
    ------
     00|1
     01|1
     10|1
     11|0

     nor
    ------
     00|1
     01|0
     10|0
     11|0

     xor
    ------
     00|0
     01|1
     10|1
     11|0


As you can verify for yourself, the expressions do indeed generate the expected values for their logical functions.


A standard XOR gate built out of NOR gates might look like this if it were to be translated directly into Circle Expressions:

    ((a(ab))((ab)b))


But we have the expression:

    (((a)(b))(ab))


We can use the logical methods described so admirably well on the Markable Mark web site to prove that the two expressions will always evaluate to the same results no matter the particular values of the 'a' and 'b' sub-expressions.

    ((a(ab))((ab)b)) = (((a)(b))(ab)) # Ugh, double check that this ain't inverted! TODO


This is a somewhat more sophisticated approach than the simple term factoring discussed below.  It is also worth noting, again, that it is possible to generate new forms of equivalent value mechanically using existing art and it is possible to evaluate the expected performance of extensionally identical expressions as instantiated into specific physical (or software-on-existing-hardware) forms according to preferred utility metrics mechanically.  (again, cf. Gödel Machines.)


TODO: https://en.wikipedia.org/wiki/Canonical_form_%28Boolean_algebra%29 "whenever performance is vital, going beyond canonical forms and doing the mind-bending Boolean algebra to make the unenhanced NOR gates do the job is well worth while."


More Circuits


If we want to choose one of four options given a two-bit binary number we can make use of these expressions (where 'a' and 'b' are the bits and "endianness" is irrelevant):

    Two-Binary-Digits-to-One-of-Four-Selector:
    (ab) (a(b)) ((a)b) ((a)(b))


For each pair of possible combinations of two binary signals only one of the above expressions will evaluate to True and the others will evaluate to False.

If we label the "inputs" 'a', 'b', 'c', and 'd' and permit only one at a time to be True, we can compose these expressions that will evaluate to the binary digits that encode our selection:

    One-of-Four-to-Two-Binary-Digits:
    ((ab)) ((ac))


Note that 'd' is ignored.  Selecting option 'd' is the same as selecting no option: 'd' "is" zero. The first expression gives the most-significant-bit while the second give the least-significant-bit.


It is trivial to construct a circle expression that denotes a "Half-Bit Adder" circuit and proves its function. A circuit that will sum two binary inputs and output the sum and a "carry" or overflow signal can be represented by the following circle expressions (taking 'a' and 'b' as the binary digits to add together):

    Half-Bit Adder:
    Sum: (((a)(b))(ab))
    Carry: ((a)(b))


As you can plainly see, the sum is given by XORing the inputs while the carry signal is just logical AND of the inputs.  In Python:


    half_bit_adder = xor(a, b), and_(a, b)


But notice that both expressions contain the common sub-expression '((a)(b))'?  In fact, the "carry" expression is a sub-expression of the "sum".  We can reflect that in the Python code to generate the expression:

    def HBA(a, b):
      carry = and_(a, b)
      return nor(carry, nor(a, b))


This function captures (and parameterizes) the creation of a Half-Bit Adder circuit, refactoring it from the first expression but creating an identical result.


While it can be fun to perform these sorts of manipulations manually it should be obvious that they are highly susceptible to automated treatment.  There is no need to refactor the expressions at the level of Python source code, I am only doing that to illustrate the process concretely.  We explore below means of manipulating the circle expressions (as Python data structures) directly using logical unification algorithms.


In order to have richer expressions across which to reason and extend the reach of our notation we can compose expressions to generate new expressions that capture the behaviour of the composition of logic circuits.


For example, in order to build a general adder circuit that can be ganged together with copies of itself to add "wider" binary numbers we must arrange to take into account a 'Cin' signal to "carry in" the carry signal from an optional previous adder circuit.  Taking a standard circuit from the existing literature we have:

    Full-Bit Adder:
    Sum: ((((((a)(b))(ab)))(Cin))((((a)(b))(ab))Cin))
    Carry: (((((((a)(b))(ab)))(Cin))((a)(b))))


And here is the resulting Truth-Table, as generated by reducing the above expressions after substituting the input values for their symbols:

     a  b Cin  Sum Cout
    [0, 0, 0] -> 0 0
    [0, 0, 1] -> 1 0
    [0, 1, 0] -> 1 0
    [0, 1, 1] -> 0 1
    [1, 0, 0] -> 1 0
    [1, 0, 1] -> 0 1
    [1, 1, 0] -> 0 1
    [1, 1, 1] -> 1 1


The expressions give the desired output behaviour.


The Python code to generate the Full Bit Adder is straightforward:

    def FBA(a, b, Cin):
      k = xor(a, b)
      return xor(k, Cin), or_(and_(k, Cin), and_(a, b))

And again, we can refactor the expressions "manually" in the Python code just by noticing and "pulling out" sub-expressions into formal calls to the generating functions (all of which are defined in terms of the nor() operation anyway. We are just drawing circles around circles.)


    def FBA(a, b, Cin):
      h = and_(a, b)
      y = nor(h, nor(a, b))
      j = and_(y, Cin)
      return nor(j, nor(y, Cin)), or_(j, h)


I should note that I originally performed this factoring by simple string replacement on the original expressions (as a string, as they appear above) substituting new string symbols for the terms I wished to factor by means of calls to the replace() method of the expression string.  This works fine but there are better methods which are discussed below.


Again, even though the resulting expressions from each form of the FBA() function are identical, the circuits represented are not.


Logically they are both composed of the same arrangement of NOR gates but if you were to construct each of the above circuits using discrete IC components that matched the operations that appear in the functions the resulting circuits would calculate the same results but with variations in, say, power consumption, speed and other factors (but one would expect the variations to be small depending on the components chosen as modern hardware is quite efficient.)


Once we have the expressions for a Full-Bit Adder circuit that can be composed with copies of itself to create "wider" adders doing so is easy:

    Sum0, Cout0 = FBA(a0, b0, Cin)
    Sum1, Cout1 = FBA(a1, b1, Cout0)

This generates the following expressions (note that we don't care about Cout0 because it is just passed back in to the carry signal of the next stage.  The output expressions are three: two bits for the sum and one for the carry):

    Sum0: ((((((a0)(b0))(a0 b0)))(Cin))((((a0)(b0))(a0 b0))Cin))

    Sum1: ((((((a1)(b1))(a1 b1)))((((((((a0)(b0))(a0 b0)))(Cin))((a0)(b0))))))
      ((((a1)(b1))(a1 b1))(((((((a0)(b0))(a0 b0)))(Cin))((a0)(b0))))))

    Cout1: (((((((a1)(b1))(a1 b1)))((((((((a0)(b0))(a0 b0)))(Cin))((a0)(b0))))))((a1)(b1))))

We can do things to these expressions to make them simpler, but first let's "run" them by trying different inputs and checking the values of the outputs.  I've done that and formatted the binary values in decimal to make it easier to read.  Note that the 'Cout1' signal indicates an "overflow" of the two bits and so counts as 4 for purposes of determining the output of the adder as a decimal number.

    a   b  Cin sum carry
    0 + 0 + 0 = 0 + 0
    1 + 0 + 0 = 1 + 0
    0 + 1 + 0 = 1 + 0
    1 + 1 + 0 = 2 + 0
    0 + 0 + 1 = 1 + 0
    1 + 0 + 1 = 2 + 0
    0 + 1 + 1 = 2 + 0
    1 + 1 + 1 = 3 + 0
    2 + 0 + 0 = 2 + 0
    3 + 0 + 0 = 3 + 0
    2 + 1 + 0 = 3 + 0
    3 + 1 + 0 = 0 + 4
    2 + 0 + 1 = 3 + 0
    3 + 0 + 1 = 0 + 4
    2 + 1 + 1 = 0 + 4
    3 + 1 + 1 = 1 + 4
    0 + 2 + 0 = 2 + 0
    1 + 2 + 0 = 3 + 0
    0 + 3 + 0 = 3 + 0
    1 + 3 + 0 = 0 + 4
    0 + 2 + 1 = 3 + 0
    1 + 2 + 1 = 0 + 4
    0 + 3 + 1 = 0 + 4
    1 + 3 + 1 = 1 + 4
    2 + 2 + 0 = 0 + 4
    3 + 2 + 0 = 1 + 4
    2 + 3 + 0 = 1 + 4
    3 + 3 + 0 = 2 + 4
    2 + 2 + 1 = 1 + 4
    3 + 2 + 1 = 2 + 4
    2 + 3 + 1 = 2 + 4
    3 + 3 + 1 = 3 + 4


The expressions give the desired output behaviour.

Digital logic designers have been developing circuits to compute mathematical and logical functions for many decades, and there is an extensive body of design that can be directly expressed and understood in terms of the Circle Language. Although we have not touched upon the Church Encoding we can see that we have here simple rules that can be combined to generate patterns that we can interpret as numbers, and patterns that can be interpreted as "doing math" to those numbers.


- -


Consider the following self-referential form:

    The Set-Reset Flip-Flop:
    q = ((qs)r)

If both 's' and 'r' are allowed to be Void-valued the expression becomes 'q = ((q))' which is a statement of the basic rule '(()) = Void'.  It (the equation, not 'q') is trivially True (stable) for either value that 'q' may assume.

If 'q' is the mark (or we can say "has the value of the mark") then the value of 's' is ignored, but the value of 'r' can "invert" the value of the whole expression if it is permitted to be the value of the mark.  Likewise, the same situation holds in a kind of symmetry when 'q' is Void-valued.  In that case, the value of 'r' is ignored but 's' can "invert" the value of the whole expression if it is permitted to assume the value of the mark.

This is a kind of "memory" circuit called a Set-Reset Flip-Flop.


- -


We can model the operation over time of a system in two essential ways: we can record a protocol and analyze it, or we can analyze the system of expressions directly.


Consider a simple "machine" with only three bits in one register, and only one "instruction" hard-coded to perform:

    a = (bc)   b = (c)   c = (a(c))


Where a, b, and c are the initial state of the three bits in the register and the "next" state is given by evaluating each expression.

We can learn the "future" state of this computer by simply re-composing the expressions with themselves to effectively create circuits that compute the future state of the system one or more "cycles" ahead.

Let's "run" our computer in this manner for a few cycles. Substituting the original expressions into themselves we get:

    a = ((c)(a(c)))  b = ((a(c)))  c = ((bc)((a(c))))


And again, after two "computing" cycles.

    a = (((a(c)))((bc)((a(c)))))

    b = (((bc)((a(c)))))

    c = (((c)(a(c)))(((bc)((a(c))))))


We can also "plug in" actual values and then simulate the system by re-applying the above expressions to the values.  Doing this for each of the possible values of a, b, and c give us the following protocols:

    0 0 0 cycle: 0
    1 1 0 cycle: 1
    0 1 0 cycle: 2
    0 1 0 cycle: 3
    0 1 0 cycle: 4

    0 0 1 cycle: 0
    0 0 1 cycle: 1
    0 0 1 cycle: 2
    0 0 1 cycle: 3
    0 0 1 cycle: 4

    0 1 0 cycle: 0
    0 1 0 cycle: 1
    0 1 0 cycle: 2
    0 1 0 cycle: 3
    0 1 0 cycle: 4

    0 1 1 cycle: 0
    0 0 1 cycle: 1
    0 0 1 cycle: 2
    0 0 1 cycle: 3
    0 0 1 cycle: 4

    1 0 0 cycle: 0
    1 1 0 cycle: 1
    0 1 0 cycle: 2
    0 1 0 cycle: 3
    0 1 0 cycle: 4

    1 0 1 cycle: 0
    0 0 0 cycle: 1
    1 1 0 cycle: 2
    0 1 0 cycle: 3
    0 1 0 cycle: 4

    1 1 0 cycle: 0
    0 1 0 cycle: 1
    0 1 0 cycle: 2
    0 1 0 cycle: 3
    0 1 0 cycle: 4

    1 1 1 cycle: 0
    0 0 0 cycle: 1
    1 1 0 cycle: 2
    0 1 0 cycle: 3
    0 1 0 cycle: 4


It seems our machine has a little bit of interesting behaviour "at the beginning" and then settles down within four cycles to one of two steady states, or "basins".  We can graph this arrangement like so:


         100 -->\
                110 -> 010
  111 -> 000 -->/

  011 -> 001


The system seems to always end up in either abc=010 or abc=001 and then stay there.


It turns out that in many cases, useful cases, we can go directly from the expression version to the protocol version, and from the protocol version to the expression version, and sometimes both ways, without having to "run" the system.


In other cases this is not possible, and in some cases no one knows (or can prove) the thing one way or another.


Another interesting question is, "What precisely are these expressions and protocols versions of?"  The answer seems to be "momentum".  Or, more precisely, "patterns in momentum".  But that hardly helps because no one knows (or can prove) what momentum "is".

- -

Consider the adder circuits.  If we were to connect the output signals to one set of the input signals and let the system "run free" we would expect nothing to happen, unless the "free" input signals were permitted to assume some value (other than all-Void, representing the number zero.)

If the "free" input were zero, then any pattern on the inputs/outputs would remain stable. But if a pattern were introduced into the "free" inputs and allowed to remain stable, the adder circuit would begin to count.

The "bound" input would be "output-plus-whatever" (where "whatever" is the pattern on the "free" input"), but the output is just the "bound" input, so the system as a whole would cycle, adding the "number" pattern in the "free" input to the "number" pattern propagating through the input/output signal "lines" in a continuous cascade.

How can we know the behaviour of the adder circuit connected to itself just by thinking about it?  We didn't compose any expressions, nor run any simulations, did we?  Perhaps our brains did so without our noticing, but if we review the above train of thought it doesn't really seems necessary, does it?

We did something like, "if a+b=c, and c = a, then c+b=c"  but that only makes sense if b=0, but we are saying that we want to know what happens if b="something-else".  It turns out that either you get what is called a "paradox" or you get what is called "time".  But of course those are just two more names for "things" nobody understands.

Nevertheless, the circuits work.


- -

A Universal Circuit?

Consider the following self-referential form, with 'Q', 'a', and 'b' initially Void-valued:

Q = ((a(Q))(Qb))

The signal 'a' changing from Void to "Something" alone has no effect, but if either 'b' or 'Q' are "brought high" while 'a' is then 'Q' will remain "on" as long as 'a' does.

On the other hand, if 'b' is "brought high" without 'a' then 'Q' cycles (at a frequency dependent upon the physical characteristics of the mechanism enacting the form.  What frequency does such a "machine" assume if it is just "imaginary", just "in your head"?  Is it possible to have an "imaginary" machine maintain a "real" cycle frequency?)

This formal equation, depending on the circumstances of analysis, functions as a switch, a memory, and an oscillator, and copies of it can be ganged together to establish any particular function imaginable.  (That last sentence may not be true, nor this one.)



- -

TODO: Composing "high-level" functions and "programming languages" out of the "low-level" circuit expressions.

If a computer is modelled as a set of bits with a selectable array of circle expression logic that transforms as much of the bit-set per cycle as we can figure out how to profitable select and transform, how do we figure out how to profitable select those circle expressions?

TODO: How do computer designers do it?  We can make switching circuits and apply different operations depending on the values of given signals.  In effect the signals "name" the operations to take, but from another point of view there is just content-free switching.  The switches and their patterns are themselves always subject to a modelling or naming process that then gives "us" (whatever we are) the "meaning" of a pattern or computation (which is just a dynamic pattern.) 

TODO: What is the relation between the circles as logical statements and the circles as schematics for logical circuits?


They seem the same in some ways and sort-of "orthagonal" in others.  With the logical view we must find steps to convert expressions into equivalent expressions that are in some way "closer" to a form that has "something" we prefer to the previous form.  With the circuit-schematic view we are confronted with direct expressions of the structure and correctness of logical networks.  There are no notational hidden aspects and there can be no "bugs" or errors.  The expressions "are" the correct circuits in the same way that the letter 'a' "is" the letter 'a' and cannot be some other letter (unless we change our minds about the meaning of that particular squiggle, but then we are hardly playing the game correctly.)


To go from form to form by logical manipulation is a mechanical process after we have figured out what to do (what program of transformations to apply to change one form into another equivalent form) but the process of figuring out a given program seems, while still having purely mechanical aspects, to partake of something more, something beyond the abilities of those things we call machines.


Also, we can construct computer programs (constellations of circle expressions, after the usage in this paper) that help us design and model logic circuits.  Computer-aided design of high-density ICs is an established field with a vast body of art and science behind it.  Since these are just circle expressions creating and modifying circle expressions, why not remove as much of the intervening abstractions and try to find a more direct path?


The patterns in the circle expressions considered as logical circuits are computing "proofs" in real-time of the results of, in effect, circle expressions considered as logical propositions.


TODO: Reworking expressions using AI with LogPy.


TODO: Shroup et. al. Imaginary Booleans and temporal logical systems.



Curious Musicians
________________________________________


Another way to think about the mark is as a musician who is so curious about sound that he or she will play his or her music whenever it gets too quiet. If you imagine such a musician alone in the Void she would begin playing her music and the Void would fill with sound.

Imagine that our musician were so entranced and curious about music that, whenever she heard another song she stops playing his or her own music to listen.

If our musician were a little funny, she might hear her own song and fail to recognize it and stop playing to listen!  But when she did the music would fade away and, as things quieted down, she would begin to play again to "fill the void."

But perhaps our musician is not so odd, and only stops to listen to the music of other musicians.  If a friend were to show up, and he had the same peculiarity, he would just sit and listen to her play.  If she stopped for some reason he would begin to play and she would listen to him until he stopped for some reason.  In effect they would resemble the Set-Reset circuit above, with the stopping of a musician being the action of the appropriate 's' or 'r' signal.

But what if there were three musicians? Hmm.

I'm not sure just what would happen then, I suppose it would depend on the fleetness of the musicians, who started to play first when someone else stopped.

We can imagine a whole ensemble of such musicians, provided with microphones and headphones, and some sort of sound-board that let each of the muscians hear some but not all of the others.

Their music will compute.

The computations performed will depend both on the arrangement of who can hear whom, and on the "reaction times", if you will, of the individual musicians (and the connections between them.)

- -

An LC resonant tank circuit paired with a negative resistor matched to the impedance of the LC circuit forms a self-ringing oscillator.  Left to itself this circuit (if powered) will "ring" from self-excitation at the resonant frequency of the LC circuit.

If we arrange an external circuit that can be switched into the resonant circuit in such a way as to dampen its self-ringing and then arrange that external dampening circuit to be activated by the "ringing" of another resonator, we will have created a form of "NOR-ish" gate.

Consider that the external dampening circuit can be made to activate (or not) depending on a sensitivity to one or more specific frequencies or range or ranges of frequencies, and that the resonators themselves can be designed to have specific frequencies, or even to be tunable.

It seems that we might be able to construct static networks of resonators interconnected by these dampeners that would function variously depending on the frequencies of the signals arising and propagating through the network.

"Bits" in this network would have frequency (color, if it helps to visualize) and could, in effect, select the operations to perform on themselves by means of carrying these different frequencies.

We would compute by a kind of music.

I have not explored this further, but I think it points the way to a kind of processing where the basic signal values are no longer simply binary but instead are perhaps sine waves or even more exotic forms.  I speculate that we may be able to build what are effectively macro-scale non-exotic quantum computers, or other exotic but practical computing machinery.

TODO: Cybernetics [is Category Theory]


Conclusion.

Remember when I said, "No more will be said here about the transcendental function of the notation."?  I lied, I will say this: All form is but folly, and you cannot find the distinction, the "horizon", between "you" and "the world", until you remember that there isn't any.


References

John Backus and Functional Programming
- - - - - - - - - - - - - - - - - - - - - -
https://en.wikipedia.org/wiki/John_Backus
"Can Programming Be Liberated From the von Neumann Style?" 1977 Turing Award Lecture
http://www.thocp.net/biographies/papers/backus_turingaward_lecture.pdf



Spencer-Brown and Laws of Form
- - - - - - - - - - - - - - - - - - - - - -
https://en.wikipedia.org/wiki/G._Spencer-Brown
https://en.wikipedia.org/wiki/Laws_of_Form


The Markable Mark
- - - - - - - - - - - - - - - - - - - - - -
George Burnett-Stuart
http://www.markability.net/


A good translation of the Tao Te Ching
- - - - - - - - - - - - - - - - - - - - - -
Star, Jonathan; Laozi "Tao Te Ching: The Definitive Edition"
ISBN 9781585420995

@book{2001tao,
  title={Tao Te Ching: The Definitive Edition},
  author={Star, J.},
  isbn={9781585420995},
  lccn={00054379},
  url={http://books.google.com/books?id=d3jXAAAAMAAJ},
  year={2001},
  publisher={Jeremy P Tarcher/Putnam}
}



A Neuroanatomist becomes Buddha (doesn't make a fuss about it.)
- - - - - - - - - - - - - - - - - - - - - -
http://www.ted.com/talks/lang/ja/jill_bolte_taylor_s_powerful_stroke_of_insight.html
https://en.wikipedia.org/wiki/Jill_Bolte_Taylor

